{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe635700-c91c-45b1-8cc1-3170963951da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries and model APIs\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import balanced_accuracy_score, f1_score, classification_report\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.utils.class_weight import compute_class_weight, compute_sample_weight\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"deep\")\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "84892bca-36b7-439f-a83d-dea2e7ec6afd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2129, 46)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and tidy the engineered feature sheet\n",
    "ctg_path = Path(\"CTG.xls\")\n",
    "assert ctg_path.exists(), f\"Expected Excel file at {ctg_path}\"\n",
    "\n",
    "def read_feature_sheet(path: Path, sheet=0):\n",
    "    \"\"\"Load the CTG feature sheet, using row 2 as headers and data from row 3 onward.\"\"\"\n",
    "    xls = pd.ExcelFile(path)\n",
    "    sheet_name = xls.sheet_names[sheet] if isinstance(sheet, int) else sheet\n",
    "    try:\n",
    "        df = pd.read_excel(xls, sheet_name=sheet_name, header=1)\n",
    "    except ValueError:\n",
    "        df = pd.read_excel(xls, sheet_name=sheet_name, header=0)\n",
    "    return df\n",
    "\n",
    "raw_features = read_feature_sheet(ctg_path, sheet=1)\n",
    "raw_features.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2c8de8ca-2a36-4b9f-a4b6-2bc74b8bed1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2115, 30),\n",
       " NSP\n",
       " 1    1647\n",
       " 2     293\n",
       " 3     175\n",
       " Name: count, dtype: int64)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean columns, drop leakage, and ensure numerics\n",
    "def tidy_sheet(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    cleaned = df.copy()\n",
    "    cleaned.columns = [str(col).strip() for col in cleaned.columns]\n",
    "    cleaned = cleaned.dropna(axis=0, how='all').dropna(axis=1, how='all')\n",
    "    cleaned = cleaned.loc[:, ~cleaned.columns.str.contains('^Unnamed', case=False)]\n",
    "    cleaned = cleaned.loc[:, ~cleaned.columns.duplicated()]\n",
    "    rename_map = {\n",
    "        'MSTV': 'mSTV',\n",
    "        'MLTV': 'mLTV',\n",
    "        'Variance ': 'Variance',\n",
    "        'TENDENCY': 'Tendency'\n",
    "    }\n",
    "    cleaned = cleaned.rename(columns={k: v for k, v in rename_map.items() if k in cleaned.columns})\n",
    "    return cleaned\n",
    "\n",
    "#Excel exports often have extra blank rows/columns, \"Unnamed\" columns, whitespace, or duplicated column names. \n",
    "#Cleaning column names and removing empty/duplicate columns makes downstream code reliable and prevents accidental features \n",
    "#like blank columns from being treated as real inputs.\n",
    "\n",
    "sheet2 = tidy_sheet(raw_features)\n",
    "target_col = 'NSP'\n",
    "label_leak_cols = ['CLASS', 'A', 'B', 'C', 'D', 'E', 'AD', 'DE', 'LD', 'FS', 'SUSP']\n",
    "feature_cols = [col for col in sheet2.columns if col not in label_leak_cols + [target_col]]\n",
    "clean_df = (\n",
    "    sheet2\n",
    "    .drop(columns=label_leak_cols, errors='ignore')\n",
    "    .dropna(axis=0, how='all')\n",
    "    .drop_duplicates()\n",
    ")\n",
    "\n",
    "# Coerce numeric columns and drop rows without labels\n",
    "numeric_cols = feature_cols\n",
    "clean_df[numeric_cols] = clean_df[numeric_cols].apply(pd.to_numeric, errors='coerce')\n",
    "clean_df = clean_df.dropna(subset=[target_col]).copy()\n",
    "clean_df[target_col] = clean_df[target_col].astype(int)\n",
    "\n",
    "X = clean_df[numeric_cols]\n",
    "y = clean_df[target_col]\n",
    "X.shape, y.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7d9c9c2e-fe6c-48c5-aaf3-939416e6ab43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   Split  Size  Normal (1) proportion  Suspect (2) proportion  \\\n",
       " 0  Train  1480                   0.78                    0.14   \n",
       " 1   Test   635                   0.78                    0.14   \n",
       " \n",
       "    Pathologic (3) proportion  \n",
       " 0                       0.08  \n",
       " 1                       0.08  ,\n",
       " {1: 0.4278693263949118, 2: 2.4065040650406506, 3: 4.043715846994536})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stratified train/test split and class weights\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "classes = np.unique(y_train)\n",
    "class_weight_values = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
    "class_weight_dict = dict(zip(classes, class_weight_values))\n",
    "sample_weight_train = compute_sample_weight(class_weight='balanced', y=y_train)\n",
    "\n",
    "distribution = (\n",
    "    pd.DataFrame({\n",
    "        'Split': ['Train', 'Test'],\n",
    "        'Size': [len(y_train), len(y_test)],\n",
    "        'Normal (1)': [np.mean(y_train == 1), np.mean(y_test == 1)],\n",
    "        'Suspect (2)': [np.mean(y_train == 2), np.mean(y_test == 2)],\n",
    "        'Pathologic (3)': [np.mean(y_train == 3), np.mean(y_test == 3)]\n",
    "    })\n",
    "    .rename(columns=lambda c: c if c in {'Split', 'Size'} else f'{c} proportion')\n",
    ")\n",
    "distribution, class_weight_dict\n",
    "\n",
    "\n",
    "#stratify=y keeps the same class proportions in train and test sets â€” critical when classes are imbalanced so that the test set is representative. \n",
    "# random_state=42 ensures reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c4e5804f-b474-4a37-afd6-0517e46cb82b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-3.0.5-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in c:\\ying\\anaconda\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\ying\\anaconda\\lib\\site-packages (from xgboost) (1.13.1)\n",
      "Downloading xgboost-3.0.5-py3-none-win_amd64.whl (56.8 MB)\n",
      "   ---------------------------------------- 0.0/56.8 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 10.5/56.8 MB 72.5 MB/s eta 0:00:01\n",
      "   -------- ------------------------------- 12.6/56.8 MB 29.2 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 23.3/56.8 MB 36.0 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 39.1/56.8 MB 46.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  56.1/56.8 MB 53.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 56.8/56.8 MB 46.4 MB/s eta 0:00:00\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-3.0.5\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2144ad-cf9c-47ce-b3d3-f5c1cd3b30aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "# Relabel classes so they start at 0\n",
    "y_train_fixed = y_train - np.min(y_train)\n",
    "y_test_fixed = y_test - np.min(y_test)\n",
    "\n",
    "# Fit grid search with the fixed labels\n",
    "grid_search.fit(X_train, y_train_fixed)\n",
    "\n",
    "print(\"Best params:\", grid_search.best_params_)\n",
    "print(\"Best CV balanced accuracy:\", grid_search.best_score_)\n",
    "\n",
    "xgb = XGBClassifier(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    scale_pos_weight=1,   # tune this for imbalance\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    eval_metric=\"mlogloss\"\n",
    ")\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [4,6,8,10],\n",
    "    'learning_rate': [0.01,0.03,0.05,0.1],\n",
    "    'n_estimators': [300,500,700,1000],\n",
    "    'subsample': [0.6,0.7,0.8,0.9,1.0],\n",
    "    'colsample_bytree': [0.6,0.7,0.8,0.9,1.0],\n",
    "    'gamma': [0,0.1,0.25,0.5],\n",
    "    'min_child_weight': [1,3,5,7]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=xgb,\n",
    "    param_grid=param_grid,\n",
    "    scoring=\"balanced_accuracy\",\n",
    "    cv=cv,\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"Best params:\", grid_search.best_params_)\n",
    "print(\"Best CV balanced accuracy:\", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7067dac-2e04-4db3-9700-9dea1b7feabb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
